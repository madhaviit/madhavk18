[{"authors":null,"categories":null,"content":"Madhav is a ambitious student studying computer science at IIT Indore. He is interested in using modern innovations for social good and research required for it. He loves debating and writing poems. He is good at table tennis, carrom and chess. He is an avid reader and occasional writer deeply interested in politics and philosophy.\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Madhav is a ambitious student studying computer science at IIT Indore. He is interested in using modern innovations for social good and research required for it. He loves debating and writing poems.","tags":null,"title":"Madhav Kadam","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://madhaviit.github.io/madhavk18/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/madhavk18/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"How my team representing IIT Indore bagged an absolute Gold Medal in Inter IIT Tech Meet 11.0 (2023 edition at IIT Kanpur) for the Mid-Prep “The Vital Extraction” Challenge by Cloudphysician. Abstract: The vital extraction challenge aims to extract a patient’s vitals from the image of an ICU monitor. Monitoring vitals is critical to providing high-quality care to patients and is essential for ensuring the best possible patient outcomes. While current guidelines state that the nurse-to-patient ratio should be 1:6, various practical issues result in a much worse scenario. This is why it is important to find newer and more efficient solutions to help solve this problem.\nWhile it would be preferable to skip the camera-based monitoring system and directly feed the vitals into a common server, we recognize the fact that this solution is not an efficient solution for ICUs around India that are already up and running. There is a need to augment the existing ICU environments in order to capture the necessary vitals from an “offline” patient monitor and feed it into an “online” server for monitoring.\nPrecisely, this is the task assigned to us. This challenge required the development of a pipeline to first detect and segment the monitor’s screen from the image and then detect, segment, and understand the various vitals present on the screen.\nPipeline We propose to solve the task with a 3-step solution, namely, Preprocessing, Vital Detection, and OCR. Lastly, we also tackled the HR Graph digitization.\nPreprocessing First, there is the segmentation of the monitor screen and separating it from the surroundings. Secondly, we have to scale the segmented screen to a uniform size for later stages while preserving the original resolution as much as possible. We apply the perspective transformation on the segmented image to obtain the resultant image.\nVital Detection This stage in the pipeline aims to detect the appropriate vitals in the cropped and transformed monitor screen. However, there was a lot of unlabelled data, and manually annotating all 9000 images was not an option. And it will never be a good solution in any real-world scenario. We identified two methods to deal with the vast amount of unlabelled data.\nOne approach was to use a customized Semi-Supervised Learning Model, which would learn from the small amount of labelled data and use it to generate pseudo labels and then true labels for the unlabelled data.\nAnother approach was to intelligently pick and manually annotate a small amount of unlabelled data and then add it to the training dataset in order to bring maximum diversity and representation to the training data without much effort. This approach was chosen because of its better results and lesser computational requirement.\nOCR Finally, we applied an OCR to extract the values of the vitals. This was not a particularly difficult task considering the standardized fonts used on patient monitors. However, there were a few misreads in the final result, such as “0” being read as “o” or “O”, “1” being read as “I”, etc. Due to some detection inaccuracies, we would also see brackets creep into the detection box. Due to segmentation faults, sometimes the numbers at the left corner of the screen, most usually the Systolic pressure in monitors which display it at the bottom left, lose the hundreds’ place digit. We fix such issues by hard-coding a few logic-based checks and corrections to get the most logical possibility for the correct value.\nHR Graph Digitization Initially, we converted the HR graph segmented image into binary. In order to selectively obtain only the graph, the longest connected pixels row-wise were saved, and the rest discarded. The 2-D binary image was then projected into a 1-D Time Series. The plot was further rescaled in the x and y variables.\nModels Used: Segmentation: We use the YOLO v8-n model for segmentation of monitor screens from input image. It gave 0.995 mAP50 and 0.987 mAP50-95 with inference time of around 120 ms.\nPreprocessing: The segmented image is then approximated into a quadrilateral (four points). We apply perspective transformation which brings vitals to more readable form.\nVital Detection: We use YOLO v8-s model for vital extraction from preprocessed image. It gave 0.988 mAP50 and 0.849 mAP50-95 with inference time of around 200 ms.\nOCR of vitals: We use Paddle OCR because of its accuracy and speed.\nSegmentation model metrics: Model Epochs Box Precision Box Recall mAP50 mAP50-95 YOLOv8n 30 1 1 0.995 0.987 YOLOv8s 85 1 1 0.995 0.987 Mask RCNN 250 0.99 0.918 Model Epochs IOU Loss Mean Accuracy Mean IOU Segformer 35 0.011 0.992 0.985 Detection model metrics: Model Epochs Box Precision Box Recall mAP50 mAP50-95 YOLOv8n 25 0.986 0.993 0.99 0.831 YOLOv8s 100 0.983 0.989 0.988 0.849 YOLOv8m 100 0.981 0.988 0.989 0.841 YOLOv7 (R) 0.991 0.996 0.993 0.83 YOLOv6 200 0.954 0.721 DETR 45 0.979 0.688 RetinaNet 20 0.595 0.286 Optical Character Recognition Model Accuracy Dataset Inference Time …","date":1678703961,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678703961,"objectID":"db0db33a43a494b5d94071a84032f348","permalink":"https://madhaviit.github.io/madhavk18/post/the-vital-extraction-challenge/","publishdate":"2023-03-13T10:39:21.709Z","relpermalink":"/madhavk18/post/the-vital-extraction-challenge/","section":"post","summary":"How my team representing IIT Indore bagged an absolute Gold Medal in Inter IIT Tech Meet 11.0 (2023 edition at IIT Kanpur) for the Mid-Prep “The Vital Extraction” Challenge by Cloudphysician.","tags":null,"title":"The Vital Extraction Challenge","type":"post"},{"authors":null,"categories":null,"content":"In this notebook my team explains how to use our pipeline for getting infernece. Imports Section !pip install ultralytics import ultralytics from ultralytics import YOLO import numpy as np import cv2 as cv !pip install \u0026#34;paddleocr==2.2\u0026#34; !pip install paddlepaddle import matplotlib.pyplot as plt from paddleocr import PaddleOCR,draw_ocr Helper Function Definition # Orders points in a particular way def order_points(pts, x, y): rect = np.zeros((4, 2), dtype = \u0026#34;float32\u0026#34;) s = pts.sum(axis = 2) rect[0] = pts[np.argmin(s)] rect[2] = pts[np.argmax(s)] diff = np.diff(pts, axis = 2) rect[1] = pts[np.argmin(diff)] rect[3] = pts[np.argmax(diff)] rect[0][0]-=5 #if rect[0][0]\u0026gt;=5 else rect[0][0] rect[0][1]-=5 #if rect[0][1]\u0026gt;=5 else rect[0][1] rect[2][0]+=5 #if rect[2][0]\u0026lt;=x-5 else rect[2][0] rect[2][1]+=5 #if rect[2][1]\u0026lt;=y-5 else rect[2][1] rect[1][0]+=5 #if rect[1][0]\u0026lt;=x-5 else rect[1][0] rect[1][1]-=5 #if rect[1][1]\u0026gt;=5 else rect[1][1] rect[3][0]-=5 #if rect[3][0]\u0026gt;=5 else rect[3][0] rect[3][1]+=5 #if rect[3][1]\u0026lt;=y-5 else rect[3][1] return rect Inference Function Extract the model files from the zip files, and upload to colab.\nThen add the filepaths.\nseg_path should contain the path to the weights of the segmentation model.\ndet_path should contain the path to the weights of the detection model.\nRun the below cell to load models. Then the inference function can be used.\nseg_model=YOLO(\u0026#39;\u0026#39;) det_model=YOLO(\u0026#39;\u0026#39;) name_dict=det_model.names ocr=PaddleOCR(use_angle_cls=True,lang=\u0026#39;en\u0026#39;) Namespace(benchmark=False, cls_batch_num=6, cls_image_shape=\u0026#39;3, 48, 192\u0026#39;, cls_model_dir=\u0026#39;/root/.paddleocr/2.2/ocr/cls/ch_ppocr_mobile_v2.0_cls_infer\u0026#39;, cls_thresh=0.9, cpu_threads=10, det=True, det_algorithm=\u0026#39;DB\u0026#39;, det_db_box_thresh=0.5, det_db_score_mode=\u0026#39;fast\u0026#39;, det_db_thresh=0.3, det_db_unclip_ratio=1.6, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_east_score_thresh=0.8, det_limit_side_len=960, det_limit_type=\u0026#39;max\u0026#39;, det_model_dir=\u0026#39;/root/.paddleocr/2.2/ocr/det/en/en_ppocr_mobile_v2.0_det_infer\u0026#39;, det_sast_nms_thresh=0.2, det_sast_polygon=False, det_sast_score_thresh=0.5, drop_score=0.5, e2e_algorithm=\u0026#39;PGNet\u0026#39;, e2e_char_dict_path=\u0026#39;./ppocr/utils/ic15_dict.txt\u0026#39;, e2e_limit_side_len=768, e2e_limit_type=\u0026#39;max\u0026#39;, e2e_model_dir=None, e2e_pgnet_mode=\u0026#39;fast\u0026#39;, e2e_pgnet_polygon=True, e2e_pgnet_score_thresh=0.5, e2e_pgnet_valid_set=\u0026#39;totaltext\u0026#39;, enable_mkldnn=False, gpu_mem=500, help=\u0026#39;==SUPPRESS==\u0026#39;, image_dir=None, ir_optim=True, label_list=[\u0026#39;0\u0026#39;, \u0026#39;180\u0026#39;], lang=\u0026#39;en\u0026#39;, layout_path_model=\u0026#39;lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config\u0026#39;, max_batch_size=10, max_text_length=25, min_subgraph_size=10, output=\u0026#39;./output/table\u0026#39;, precision=\u0026#39;fp32\u0026#39;, process_id=0, rec=True, rec_algorithm=\u0026#39;CRNN\u0026#39;, rec_batch_num=6, rec_char_dict_path=\u0026#39;/usr/local/lib/python3.8/dist-packages/paddleocr/ppocr/utils/en_dict.txt\u0026#39;, rec_char_type=\u0026#39;ch\u0026#39;, rec_image_shape=\u0026#39;3, 32, 320\u0026#39;, rec_model_dir=\u0026#39;/root/.paddleocr/2.2/ocr/rec/en/en_number_mobile_v2.0_rec_infer\u0026#39;, save_log_path=\u0026#39;./log_output/\u0026#39;, show_log=True, table_char_dict_path=None, table_char_type=\u0026#39;en\u0026#39;, table_max_len=488, table_model_dir=None, total_process_num=1, type=\u0026#39;ocr\u0026#39;, use_angle_cls=True, use_dilation=False, use_gpu=True, use_mp=False, use_pdserving=False, use_space_char=True, use_tensorrt=False, vis_font_path=\u0026#39;./doc/fonts/simfang.ttf\u0026#39;, warmup=True) # The function below takes in a path to an image, or a numpy array and returns a dictionary with the values read from the image def inference(image): name_dict=det_model.names # Initialize dictionaries labels={} confidence={\u0026#34;HR\u0026#34;:0, \u0026#34;RR\u0026#34;:0, \u0026#39;SBP\u0026#39;:0, \u0026#34;DBP\u0026#34;:0, \u0026#34;SPO2\u0026#34;:0, \u0026#34;MAP\u0026#34;:0, \u0026#39;HR_W\u0026#39;:0} if isinstance(image,str): img=cv.imread(image) elif isinstance(image,np.ndarray): img=image # Get segmentation predictions results = seg_model.predict(img,verbose=False) # If segmention masks returned are not empty (i.e if image is not already segmented) if len(results[0])!=0: # Get segments and crop image t=results[0].masks.segments t=np.array(t) t[0][:,0]=t[0][:,0]*(img.shape[1]) t[0][:,1]=t[0][:,1]*(img.shape[0]) c=np.int32(t[0].reshape(-1,1,2)) # Approximate segmentation segments with a quadrilateral u=cv.approxPolyDP(c,0.04 * cv.arcLength(c, True), True) # Order corners of the quadrilateral in a particular order u=order_points(u,img.shape[1],img.shape[0]) size_x=720 size_y=512 pts2=np.float32([[0,0], [size_x, 0],[size_x, size_y], [0, size_y]]) # Apply perspective transform on the image to get an image to perform detection on matrix = cv.getPerspectiveTransform(u.astype(np.float32), pts2) seg = cv.warpPerspective(img, matrix, (size_x, size_y)) # If the image is already segmented then just run the image through detection else: seg=img # Perform detetection with the detection model det=det_model.predict(seg,verbose=False)[0].boxes detxy=(det.xyxy.numpy()).astype(np.int32) detcls=det.cls.numpy().astype(np.int32) detconf=det.conf.numpy() # Iterate over regions of interest detected and acquire OCR results, use highest confidence for each class for i,j in enumerate(detcls): if name_dict[j] !=\u0026#39;HR_W\u0026#39; and …","date":1678492800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678492800,"objectID":"6a6ac0dabb5aa13c1d9831739bffa7d9","permalink":"https://madhaviit.github.io/madhavk18/post/vitals_notebook/","publishdate":"2023-03-11T00:00:00Z","relpermalink":"/madhavk18/post/vitals_notebook/","section":"post","summary":"In this notebook my team explains how to use our pipeline for getting infernece. Imports Section !pip install ultralytics import ultralytics from ultralytics import YOLO import numpy as np import cv2 as cv !","tags":null,"title":"Code for inference in Vitals Extraction Chaallenge","type":"post"},{"authors":["Madhav Kadam"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://madhaviit.github.io/madhavk18/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/madhavk18/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://madhaviit.github.io/madhavk18/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/madhavk18/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://madhaviit.github.io/madhavk18/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/madhavk18/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://madhaviit.github.io/madhavk18/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/madhavk18/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Madhav Kadam","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://madhaviit.github.io/madhavk18/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/madhavk18/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Madhav Kadam","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://madhaviit.github.io/madhavk18/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/madhavk18/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]